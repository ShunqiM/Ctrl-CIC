recursive_load_config: True  # recursive load all "_default.yaml" files (from current level to all upper-level folders)

use_cached_feature: True
use_image_embs: True
add_input_mask: True
# For now, what this really do is letting dataset returns the mask of the context 
mask_as_labels: True

note: weight_predictor
trainer:
  params:
    trainer_class: src.pipeline.CustomTrainer
    max_steps: 1000000
    # max_steps: 262144
    per_device_train_batch_size: 20
    per_device_eval_batch_size: 16
    remove_unused_columns: False
    dataloader_num_workers: 16
    # report_to: 'none'
    # output_dir: tmp
    output_dir: ${CKPT}/CIC/experiments
    # output_dir: ${CKPT}/CIC/test
    fp16: False
    do_eval: True
    evaluation_strategy: steps
    eval_steps: 10000
    logging_strategy: steps
    logging_steps: 1000
    save_steps: 10000
    load_best_model_at_end: True
    learning_rate: 5.e-5 # 5e-5 is the default
    save_total_limit: 30


dataset:
  trainset:
    # callable: DummyDataset
    callable: Web2MFTDataset
    params:
      # csv_path: /mnt/sdb/PREP/MMWebpage/val_image_dict.csv
      csv_name: train_image_dict_v9.csv
      feature_matrix_name: train_image_feature_matrix_full.pt
      max_src_len: 511 # leave one for image embedding
      max_tgt_len: 128
      mask_dir: 't5-large_mean_512'
      normalise_score: fix_scale
      pad_image: True

  validset:
    callable: Web2MFTDataset
    params:
      csv_name: val_image_dict_v7.csv
      feature_matrix_name: val_image_feature_matrix.pt
      max_src_len: 511
      max_tgt_len: 128
      mask_dir: 't5-large_mean_512'
      normalise_score: fix_scale
      pad_image: True


  testset:
    callable: Web2MFTDataset

# LongT5 with fp16 does not work correctly as they are trained on bf16, and it's not feasible at our set up, 
# https://github.com/huggingface/transformers/issues/17978
model:
  callable: PretrainedLongT5forTokenRegression
  params:
    pretrained_model_name_or_path: google/long-t5-tglobal-base
    loss: 'cross_entropy'
  image_model: vit
  image_model_name: google/vit-base-patch16-224-in21k

# Note that till now none of the below are used, but the hf default
optimizer:
  type: AdamW
  adamw_params:
    lr: 1.0e-4
    betas: [0.9, 0.98]
    weight_decay: 1.0e-5
  adafactor_params:
    lr: 1.0e-4
    weight_decay: 1.0e-5
    beta1: null
