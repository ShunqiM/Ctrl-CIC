We would like to request your feedback on the performance of two AI assistants in response to the controllable contextual image captioning task, where captions for an image will be generated based on the overall context and specific contextual highlights provided.

Evaluation Steps:
1. You will be given the image, [Context Section] and [Highlighted Segments], followed by controllable contextual captions provided by the two assistant [ASSISTANT1 Caption] and [ASSISTANT2 Caption]: The context section will be given as the combination of a page title, a section title, and a section body. The highlighted segments are parts of the section body which will be given as words, phrases, or sentences, separated by line breaks.
2. You will thoroughly read the [Context Section] and [Highlighted Segments] provided, and carefully examine the [Image].
3. You will read the caption generated by the AI assistant.
4. You will evaluate the controllable contextual image captioning quality of the two AI assistants, in terms of 4 aspects (which are "Relevance with Context", "Relevance with Highlight", "Consistency with Image", and "Overall Quality") - see below for individual criteria of these aspects. Each criterion should be considered in isolation to provide a clear and focused evaluation.
5. You will complete the following five sections IN ORDER (namely, [ASSISTANT1-Reasoning], [ASSISTANT2-Reasoning], [Comparison-Reasoning], [ASSISTANT1-Score], and [ASSISTANT2-Score])

Evaluation Criteria:
- Relevance with Context: This metric rates how relevant the caption is to the given context. It assesses whether the caption pertains to and is appropriate for the contextual information provided, without necessarily reflecting the entire context. Captions should be scored based on their pertinence and the degree to which they relate to the context. Annotators should deduct points for captions that do not relate to or ignore the context. Higher scores should be awarded to captions that show a clear and significant relevance to the context.
- Relevance with Highlight: This metric evaluates how well the caption aligns with the highlighted segments provided. The caption should accurately reflect the information contained in the highlighted segments, ensuring that it is relevant and integrated into the overall caption. Annotators are advised to penalize captions that fail to address the highlighted segments or do so in a manner that does not give them adequate prominence or relevance.
- Consistency with Image: This metric evaluates the accuracy with which the caption represents elements or themes that are verifiably present in the image, based on the provided image descriptions. The caption should not introduce content that is clearly absent from the image. It needs to maintain a clear and direct connection to the key elements depicted in the image. Annotators should deduct points for captions that include inconsistencies or introduce elements not discernible in the image. Higher scores should be reserved for captions that are faithful to the image's visible content.
- Overall Quality: This metric assesses the caption's overall effectiveness in the CCIC task, emphasizing its coherence with the overall context, alignment with the image, and relevance to the highlighted segment. A high-quality caption should seamlessly integrate these elements, providing an accurate, informative, and engaging description of the image that resonates with the given context and highlights.


HINT: 
1. [ASSISTANT1-Reasoning] and [ASSISTANT2-Reasoning] will be used to record your reasoning and comments on the controllable contextual image caption generation quality of the two AI assistants, respectively; 
2. [Comparison-Reasoning] will be used to record your feedback (with supporting evidence) for comparisions between the two AI assistants, which will be used to support the below two marking sections;
3. [ASSISTANT1-Score] and [ASSISTANT2-Score] will be used to record your controllable contextual image caption scores of the two AI assistants, respectively. Each assistant receives an integer score on a scale of 1 to 5 for each criteria, where a higher score indicates better performance according to the evaluation criteria.
4. Below is an example for requested output format of measuring one of the assistants: 

[ASSISTANT1-Reasoning] (*example):
- Relevance with Context: some feedback with supporting evidance...
- Relevance with Highlight: some feedback with supporting evidance...
- Consistency with Image: some feedback with supporting evidance...
- Overall Quality: some feedback with supporting evidance...


[ASSISTANT1-Score] (*example):
- Relevance with Context: 3
- Relevance with Highlight: 1
- Consistency with Image: 4
- Overall: 2



-----------------Evaluation Starts----------------------

[Context Section]:
{{Document}}

[Highlighted Segments]:
{{Highlight}}
[ASSISTANT1 Caption]:
{{Caption_1}}

[ASSISTANT2 Caption]:
{{Caption_2}}



Please make sure you read and understand these instructions carefully, and complete the following five sections IN ORDER: (1) firstly reasons via "[ASSISTANT1-Reasoning]:" and "[ASSISTANT2-Reasoning]:"; (2) secondly compares two assistants via "[Comparison-Reasoning]:"; and (3) finally marks them via "[ASSISTANT1-Score]:" and "[ASSISTANT2-Score]:".
